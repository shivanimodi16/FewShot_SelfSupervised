{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pretrained_cifar100_extract_features.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "lYF8vPYqcJ-6"
      },
      "source": [
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
        "\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor, Lambda, Compose\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import random_split\n",
        "from torchvision.utils import make_grid\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets.utils import download_url\n",
        "from google.colab import drive\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "import numpy as np\n",
        "from google.colab import files\n",
        "from torchvision import models\n",
        "from torchsummary import summary\n",
        "from glob import glob\n",
        "import re\n",
        "from itertools import compress\n",
        "import pandas as pd\n",
        "from torchvision.models.feature_extraction import create_feature_extractor\n",
        "\n",
        "torch.manual_seed(0)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GKudmFIEOUSM"
      },
      "source": [
        "drive.mount('/content/gdrive')\n",
        "files.view('/content/gdrive/My Drive/Colab Notebooks/train_checkpoints/')\n",
        "!git clone https://github.com/samirchar/selfSupervised_fewShot.git\n",
        "from selfSupervised_fewShot.dataprep import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p5EJuoxjkrzA"
      },
      "source": [
        "target_dataset = 'STL10'\n",
        "source_dataset = 'CIFAR100'\n",
        "\n",
        "img_size = 32\n",
        "train_batch_size = 32\n",
        "test_batch_size = 64\n",
        "num_workers = 2\n",
        "\n",
        "source_root = f'/content/gdrive/My Drive/Colab Notebooks/train_checkpoints/resnet18_{source_dataset.lower()}'\n",
        "lincls_path = f'{source_root}/lincls_on_{target_dataset.lower()}'\n",
        "if not os.path.exists(lincls_path):\n",
        "  os.mkdir(lincls_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VEpu8M1KY5CV"
      },
      "source": [
        "#Get mean and std from training set of source data\n",
        "mean = np.load(os.path.join(source_root,f'{source_dataset.lower()}_train_mean.npy'))\n",
        "std = np.load(os.path.join(source_root,f'{source_dataset.lower()}_train_std.npy'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aXFG7YLFbJ5I"
      },
      "source": [
        "dataset_class = getattr(torchvision.datasets,target_dataset)\n",
        "#train_idx, train_sampler, valid_idx, val_sampler = train_val_samplers(full_train_size,val_size)\n",
        "\n",
        "#Train/test transforms\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.Resize(32),\n",
        "    transforms.RandomCrop(img_size, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean,std),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.Resize(32),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean,std),\n",
        "])\n",
        "\n",
        "if target_dataset=='STL10':\n",
        "  #Read train/val/test\n",
        "  trainset = dataset_class(  \n",
        "      root='data',\n",
        "      split='train',\n",
        "      download=True,\n",
        "      transform=transform_train\n",
        "      )\n",
        "  \n",
        "  testset = dataset_class(\n",
        "      root='data',\n",
        "      split='test',\n",
        "      download=True,\n",
        "      transform=transform_test\n",
        "      )\n",
        "else:\n",
        "  #Read train/val/test\n",
        "  trainset = dataset_class(  \n",
        "      root='data',\n",
        "      train=True,\n",
        "      download=True,\n",
        "      transform=transform_train\n",
        "      )\n",
        "\n",
        "  testset = dataset_class(\n",
        "      root='data',\n",
        "      train=False,\n",
        "      download=True,\n",
        "      transform=transform_test\n",
        "      )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UiasBYL6ZuXB"
      },
      "source": [
        "#Create data loaders\n",
        "trainloader = torch.utils.data.DataLoader(\n",
        "    trainset,\n",
        "    batch_size=train_batch_size,\n",
        "    num_workers=num_workers,\n",
        "    shuffle = True)\n",
        "\n",
        "testloader = torch.utils.data.DataLoader(\n",
        "    testset,\n",
        "    batch_size=test_batch_size,\n",
        "    shuffle=False,\n",
        "    num_workers=num_workers)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ud2WdqVlBMK_"
      },
      "source": [
        "#Load pretrained model on CIFAR100\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "resnet18 = resnet18_small(num_classes=100)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "if device == 'cuda':\n",
        "    resnet18 = torch.nn.DataParallel(resnet18)\n",
        "    cudnn.benchmark = True\n",
        "\n",
        "resnet18,_,_,_,_ = load_ckpt(resnet18,None,None,specific_model = 'best',root = source_root)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "phr0RoQsJkXX"
      },
      "source": [
        "X_train,y_train = feature_extractor(resnet18.module,'backbone.avgpool',trainloader,device)\n",
        "X_test,y_test = feature_extractor(resnet18.module,'backbone.avgpool',testloader,device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hv_ISSf9r7PH"
      },
      "source": [
        "np.save('X_train.npy',X_train)\n",
        "np.save('X_test.npy',X_test)\n",
        "\n",
        "np.save('y_train.npy',y_train)\n",
        "np.save('y_test.npy',y_test)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}