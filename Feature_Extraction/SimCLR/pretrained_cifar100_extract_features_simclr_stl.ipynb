{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lYF8vPYqcJ-6"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
        "\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor, Lambda, Compose\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import random_split\n",
        "from torchvision.utils import make_grid\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets.utils import download_url\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "import numpy as np\n",
        "from torchvision import models\n",
        "from torchsummary import summary\n",
        "from glob import glob\n",
        "import re\n",
        "from itertools import compress\n",
        "import pandas as pd\n",
        "from torchvision.datasets import STL10\n",
        "from torchvision.models.feature_extraction import create_feature_extractor\n",
        "torch.manual_seed(0)\n",
        "from torchvision.models import resnet18, resnet34"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim.lr_scheduler import LambdaLR\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import CIFAR100\n",
        "from torchvision.models import resnet18, resnet34\n",
        "from torchvision import transforms\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "vN7zVwjcBmS_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "pwX8r4afzZ7z",
        "outputId": "398967f4-2d60-4d76-f6ee-72209cd2654d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'0.11.1+cu111'"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "torchvision.__version__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GKudmFIEOUSM",
        "outputId": "3c327013-d71e-48f9-b44c-d24ca7408eb6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'selfSupervised_fewShot'...\n",
            "remote: Enumerating objects: 28, done.\u001b[K\n",
            "remote: Counting objects: 100% (6/6), done.\u001b[K\n",
            "remote: Compressing objects: 100% (5/5), done.\u001b[K\n",
            "remote: Total 28 (delta 0), reused 6 (delta 0), pack-reused 22\u001b[K\n",
            "Unpacking objects: 100% (28/28), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/samirchar/selfSupervised_fewShot.git\n",
        "from selfSupervised_fewShot.dataprep import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p5EJuoxjkrzA"
      },
      "outputs": [],
      "source": [
        "target_dataset = 'STL10'\n",
        "source_dataset = 'CIFAR100'\n",
        "\n",
        "img_size = 32\n",
        "train_batch_size = 32\n",
        "batch_size = 512\n",
        "num_workers = 2\n",
        "val_size = 5000\n",
        "full_train_size = 50000 #Could be automatic\n",
        "\n",
        "source_root = f'simclr_{source_dataset.lower()}'\n",
        "if not os.path.exists(source_root):\n",
        "  os.mkdir(source_root)\n",
        "lincls_path = f'{source_root}/lincls_on_{target_dataset.lower()}'\n",
        "if not os.path.exists(lincls_path):\n",
        "  os.mkdir(lincls_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HqIXot_7zZ71"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class SimCLR(nn.Module):\n",
        "    def __init__(self, base_encoder, projection_dim=128):\n",
        "        super().__init__()\n",
        "        self.enc = base_encoder(pretrained=False)  # load model from torchvision.models without pretrained weights.\n",
        "        self.feature_dim = self.enc.fc.in_features\n",
        "\n",
        "        # Customize for CIFAR10. Replace conv 7x7 with conv 3x3, and remove first max pooling.\n",
        "        # See Section B.9 of SimCLR paper.\n",
        "        self.enc.conv1 = nn.Conv2d(3, 64, 3, 1, 1, bias=False)\n",
        "        self.enc.maxpool = nn.Identity()\n",
        "        self.enc.fc = nn.Identity()  # remove final fully connected layer.\n",
        "\n",
        "        # Add MLP projection.\n",
        "        self.projection_dim = projection_dim\n",
        "        self.projector = nn.Sequential(nn.Linear(self.feature_dim, 2048),\n",
        "                                       nn.ReLU(),\n",
        "                                       nn.Linear(2048, projection_dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        feature = self.enc(x)\n",
        "        projection = self.projector(feature)\n",
        "        return feature, projection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pJY_JXEuzZ72"
      },
      "outputs": [],
      "source": [
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "\n",
        "class CIFAR10Pair(STL10):\n",
        "    \"\"\"Generate mini-batche pairs on CIFAR10 training set.\"\"\"\n",
        "    def __getitem__(self, idx):\n",
        "        img, target = self.data[idx], self.targets[idx]\n",
        "        img = Image.fromarray(img)  # .convert('RGB')\n",
        "        imgs = [self.transform(img), self.transform(img)]\n",
        "        return torch.stack(imgs), target  # stack a positive pair\n",
        "\n",
        "\n",
        "def nt_xent(x, t=0.5):\n",
        "    x = F.normalize(x, dim=1)\n",
        "    x_scores =  (x @ x.t()).clamp(min=1e-7)  # normalized cosine similarity scores\n",
        "    x_scale = x_scores / t   # scale with temperature\n",
        "\n",
        "    # (2N-1)-way softmax without the score of i-th entry itself.\n",
        "    # Set the diagonals to be large negative values, which become zeros after softmax.\n",
        "    x_scale = x_scale - torch.eye(x_scale.size(0)).to(x_scale.device) * 1e5\n",
        "\n",
        "    # targets 2N elements.\n",
        "    targets = torch.arange(x.size()[0])\n",
        "    targets[::2] += 1  # target of 2k element is 2k+1\n",
        "    targets[1::2] -= 1  # target of 2k+1 element is 2k\n",
        "    return F.cross_entropy(x_scale, targets.long().to(x_scale.device))\n",
        "\n",
        "\n",
        "def get_lr(step, total_steps, lr_max, lr_min):\n",
        "    \"\"\"Compute learning rate according to cosine annealing schedule.\"\"\"\n",
        "    return lr_min + (lr_max - lr_min) * 0.5 * (1 + np.cos(step / total_steps * np.pi))\n",
        "\n",
        "\n",
        "# color distortion composed by color jittering and color dropping.\n",
        "# See Section A of SimCLR: https://arxiv.org/abs/2002.05709\n",
        "def get_color_distortion(s=0.5):  # 0.5 for CIFAR100 by default\n",
        "    # s is the strength of color distortion\n",
        "    color_jitter = transforms.ColorJitter(0.8*s, 0.8*s, 0.8*s, 0.2*s)\n",
        "    rnd_color_jitter = transforms.RandomApply([color_jitter], p=0.8)\n",
        "    rnd_gray = transforms.RandomGrayscale(p=0.2)\n",
        "    color_distort = transforms.Compose([rnd_color_jitter, rnd_gray])\n",
        "    return color_distort\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 83,
          "referenced_widgets": [
            "a9420b10b67f400ebcbad715b82b6eff",
            "fdcddc4ee9014641802c4510288792de",
            "67bef0bd12884e399c66aece8bf7ab61",
            "9ed01866c035471f8b3da3d5234bc3e7",
            "238a8d0bda174f19a4c02c591bf47ddd",
            "c0ba36f522924563ba51f2456a50adb0",
            "352ec2257af146a1b063caf8205d5fc8",
            "efba2dc7d00443fabca083107cff6e97",
            "68ebe35093164fda9d326726a0071ebd",
            "dc6bdf6fa7164c01927739b26adfb682",
            "77fbc65789ed4de8beb450c65b60de96"
          ]
        },
        "id": "8Ly407hvzZ73",
        "outputId": "f9416eeb-aa16-4feb-f10f-df1f0aa6f5b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://ai.stanford.edu/~acoates/stl10/stl10_binary.tar.gz to ./data/stl10_binary.tar.gz\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a9420b10b67f400ebcbad715b82b6eff",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/2640397119 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/stl10_binary.tar.gz to ./data\n"
          ]
        }
      ],
      "source": [
        "from torchvision.transforms.transforms import Resize\n",
        "train_transform = transforms.Compose([transforms.Resize(32),\n",
        "                                      transforms.RandomResizedCrop(32,scale=(0.2, 1.)),\n",
        "                                          transforms.RandomHorizontalFlip(),\n",
        "                                          get_color_distortion(s=0.5),\n",
        "                                          transforms.ToTensor(),\n",
        "                                          transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))])\n",
        "                                        \n",
        "data_dir =  './data' # get absolute path of data dir\n",
        "\n",
        "trainset = STL10(root=data_dir,\n",
        "                            split = \"train\",\n",
        "                            transform=train_transform,\n",
        "                            download=True)\n",
        "\n",
        "train_loader = DataLoader(trainset,\n",
        "                              batch_size=512,\n",
        "                              shuffle=True,\n",
        "                              num_workers=0,\n",
        "                              drop_last=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xdjq13YGzZ74"
      },
      "outputs": [],
      "source": [
        "test_transform = transforms.Compose([\n",
        "                                     transforms.ToTensor(),\n",
        "                                     transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))])\n",
        "\n",
        "\n",
        "test_set = STL10(root=data_dir, split=\"test\", transform=test_transform, download=False)\n",
        "\n",
        "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SimCLRLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        \n",
        "    def nt_xent(self,x,t=0.5):\n",
        "        x = F.normalize(x, dim=1)\n",
        "        x_scores =  (x @ x.t()).clamp(min=1e-7)  # normalized cosine similarity scores\n",
        "        x_scale = x_scores / t   # scale with temperature\n",
        "\n",
        "        # (2N-1)-way softmax without the score of i-th entry itself.\n",
        "        # Set the diagonals to be large negative values, which become zeros after softmax.\n",
        "        x_scale = x_scale - torch.eye(x_scale.size(0)).to(x_scale.device) * 1e5\n",
        "\n",
        "        # targets 2N elements.\n",
        "        targets = torch.arange(x.size()[0])\n",
        "        targets[::2] += 1  # target of 2k element is 2k+1\n",
        "        targets[1::2] -= 1  # target of 2k+1 element is 2k\n",
        "        return F.cross_entropy(x_scale, targets.long().to(x_scale.device))\n",
        "\n",
        "    def forward(self,x,t=0.5):\n",
        "        return nt_xent(x,t)"
      ],
      "metadata": {
        "id": "WWj_SEhlExon"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LinModel(nn.Module):\n",
        "    \"\"\"Linear wrapper of encoder.\"\"\"\n",
        "    def __init__(self, encoder: nn.Module, feature_dim: int, n_classes: int):\n",
        "        super().__init__()\n",
        "        self.enc = encoder\n",
        "        self.feature_dim = feature_dim\n",
        "        self.n_classes = n_classes\n",
        "        self.lin = nn.Linear(self.feature_dim, self.n_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.lin(self.enc(x))"
      ],
      "metadata": {
        "id": "EZSVNKZ6l9Xx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ud2WdqVlBMK_",
        "outputId": "f3a98c54-f9d1-4874-b6ad-4fd56aa6a9f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SimCLR(\n",
            "  (enc): ResNet(\n",
            "    (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu): ReLU(inplace=True)\n",
            "    (maxpool): Identity()\n",
            "    (layer1): Sequential(\n",
            "      (0): BasicBlock(\n",
            "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (1): BasicBlock(\n",
            "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (layer2): Sequential(\n",
            "      (0): BasicBlock(\n",
            "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (downsample): Sequential(\n",
            "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (1): BasicBlock(\n",
            "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (layer3): Sequential(\n",
            "      (0): BasicBlock(\n",
            "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (downsample): Sequential(\n",
            "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (1): BasicBlock(\n",
            "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (layer4): Sequential(\n",
            "      (0): BasicBlock(\n",
            "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (downsample): Sequential(\n",
            "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (1): BasicBlock(\n",
            "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "    (fc): Identity()\n",
            "  )\n",
            "  (projector): Sequential(\n",
            "    (0): Linear(in_features=512, out_features=2048, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=2048, out_features=128, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "backbone = 'resnet18' # or resnet34, resnet50\n",
        "projection_dim = 128 # \"[...] to project the representation to a 128-dimensional latent space\"\n",
        "#Load pretrained model on CIFAR100\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "#resnet18 = resnet18_small(num_classes=100)\n",
        "base_encoder = eval('resnet18')\n",
        "model = SimCLR(base_encoder, projection_dim=128).cuda()\n",
        "model.load_state_dict(torch.load('simclr_best_resnet18.pt'))\n",
        "criterion = SimCLRLoss()\n",
        "print(model)\n",
        "#print(simclr_model)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  # SimCLR training\n",
        "#     model.train()\n",
        "#     optimal_loss = 1e5\n",
        "#     for epoch in range(1, epochs + 1):\n",
        "#         loss_meter = AverageMeter(\"SimCLR_loss\")\n",
        "#         train_bar = tqdm(train_loader)\n",
        "        \n",
        "#         for x, y in train_bar:\n",
        "#             sizes = x.size()\n",
        "#             x = x.view(sizes[0] * 2, sizes[2], sizes[3], sizes[4]).cuda(non_blocking=True)\n",
        "\n",
        "#             optimizer.zero_grad()\n",
        "#             feature, rep = model(x)\n",
        "#             loss = nt_xent(rep, 0.5)\n",
        "#             loss.backward()\n",
        "#             optimizer.step()\n",
        "#             scheduler.step()\n",
        "\n",
        "#             loss_meter.update(loss.item(), x.size(0))\n",
        "#             train_bar.set_description(\"Train epoch {}, SimCLR loss: {:.4f}\".format(epoch, loss_meter.avg))\n",
        "#         train_loss.append(loss_meter.avg)    \n",
        "#         if loss_meter.avg < optimal_loss:\n",
        "#             optimal_loss = loss_meter.avg\n",
        "#             logger.info(\"==> New best results\")\n",
        "#             torch.save(model.state_dict(), 'simclr_best_{}.pt'.format(backbone))"
      ],
      "metadata": {
        "id": "Wyu_NcnIc5-i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def feature_extractor2(model,layer_name,dataset,device,return_target = True):\n",
        "  return_nodes = {layer_name:'output'}\n",
        "  extractor = create_feature_extractor(model,return_nodes)\n",
        "\n",
        "  extracted_features = []\n",
        "  targets_list = []\n",
        "  with torch.no_grad():\n",
        "    for inputs, targets in dataset:\n",
        "      inputs, targets = inputs.to(device), targets.to(device)\n",
        "      sizes = inputs.size()\n",
        "      inputs = inputs.view(sizes[0] * 2, sizes[2], sizes[3], sizes[4]).cuda(non_blocking=True)\n",
        "      features = extractor(inputs)\n",
        "      print(\"features\",features['output'].shape)\n",
        "      # squeeze 1024 ,512\n",
        "      extracted_features.append(features['output'].squeeze())\n",
        "      targets_list.append(targets)\n",
        "\n",
        "  extracted_features = torch.concat(extracted_features,dim=0)\n",
        "  targets = torch.concat(targets_list,dim=0)\n",
        "  \n",
        "  if return_target:\n",
        "    return extracted_features.cpu().numpy(),targets.cpu().numpy()\n",
        "  \n",
        "  return extracted_features.cpu().numpy()\n",
        "  #return None, None"
      ],
      "metadata": {
        "id": "2CzeyioMcTiS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "phr0RoQsJkXX"
      },
      "outputs": [],
      "source": [
        "X_train,y_train = feature_extractor(model,'enc.avgpool',train_loader,device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_test,y_test = feature_extractor(model,'enc.avgpool',test_loader,device)"
      ],
      "metadata": {
        "id": "8YCTUwrzdk1W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hv_ISSf9r7PH"
      },
      "outputs": [],
      "source": [
        "np.save('X_train_STL.npy',X_train)\n",
        "\n",
        "np.save('X_test_STL.npy',X_test)\n",
        "\n",
        "np.save('y_train_STL.npy',y_train)\n",
        "np.save('y_test_STL.npy',y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T0Dp2WELsEsZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f095c25-fe1b-4655-b5fd-3ada779092a0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4608, 512)"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ],
      "source": [
        "X_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_test.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D7XQWWr5l-wq",
        "outputId": "2a9821ed-acc3-47fc-e7aa-7322430d8516"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8000, 512)"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "jVY32_LmmBMY"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "pretrained_cifar100_extract_features_simclr_stl.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.11"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a9420b10b67f400ebcbad715b82b6eff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_fdcddc4ee9014641802c4510288792de",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_67bef0bd12884e399c66aece8bf7ab61",
              "IPY_MODEL_9ed01866c035471f8b3da3d5234bc3e7",
              "IPY_MODEL_238a8d0bda174f19a4c02c591bf47ddd"
            ]
          }
        },
        "fdcddc4ee9014641802c4510288792de": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "67bef0bd12884e399c66aece8bf7ab61": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_c0ba36f522924563ba51f2456a50adb0",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_352ec2257af146a1b063caf8205d5fc8"
          }
        },
        "9ed01866c035471f8b3da3d5234bc3e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_efba2dc7d00443fabca083107cff6e97",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 2640397119,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 2640397119,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_68ebe35093164fda9d326726a0071ebd"
          }
        },
        "238a8d0bda174f19a4c02c591bf47ddd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_dc6bdf6fa7164c01927739b26adfb682",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 2640397312/? [02:21&lt;00:00, 18282556.30it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_77fbc65789ed4de8beb450c65b60de96"
          }
        },
        "c0ba36f522924563ba51f2456a50adb0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "352ec2257af146a1b063caf8205d5fc8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "efba2dc7d00443fabca083107cff6e97": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "68ebe35093164fda9d326726a0071ebd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "dc6bdf6fa7164c01927739b26adfb682": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "77fbc65789ed4de8beb450c65b60de96": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}